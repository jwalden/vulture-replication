{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Test: Bestimmung Precision / Recall\n",
    "Um die Klassifikation, und damit auch von dem Aufbau unserer Matrix, zu testen, bestimmen wir Precision und Recall.\n",
    "\n",
    "* Precision: Wieviele der zugeordneten Samples sind tatsächlich relevant? (true pos.) / (true pos. + false pos.)\n",
    "* Recall: Wieviele der tatsächlich relevanten Samples wurden gefunden / zugeordnet? (true pos.) / (true pos. + false neg.)\n",
    " * Tiefe Precision / hoher Recall: Es werden viele Samples zugeordnet, von denen jedoch viele falsch sind\n",
    " * Hohe Precision / tiefer Recall: Es werden nur wenige Samples zugeordnet, von denen jedoch die meisten stimmen.\n",
    "<img src=\"img/Precisionrecall.png\" alt=\"Drawing\" style=\"width: 200px\"/>\n",
    "\n",
    "## Optimierung\n",
    "Mit diesem Test versuchen wir die Konfiguration der SVM möglichst gut auf unsere Datenstruktur auszurichten.\n",
    "### Kernel\n",
    "Der Kernel definiert, mit welcher Art Funktion die Datenpunkte im 2D-Raum voneinander getrennt werden. Standardmässig wird dazu eine lineare Funktion (linear kernel) verwendet. Wenn die Punkte im training set nicht durch eine Gerade trennbar sind, versucht man sie in einer höheren Dimension voneinander zu trennen. Die erreicht man durch andere kernel-Funktionen (z.B. Polynomial oder RBF).\n",
    "Bei vielen Features, wie es bei unseren Daten der Fall ist, bringt eine höhere Dimension keine Vorteile, ist aber sehr teuer. Entsprechend verwenden wir einen linearen Kernel.\n",
    "\n",
    "#### Parameter\n",
    "Der lineare Kernel verlangt ausschliesslich einen Penalty Parameter C. Beim einem hohen C versucht der Classifier die Ebene so zu legen, dass möglichst wenige Datenpunkte aus dem training set falsch klassifiziert sind. Dafür nimmt er ein kleineres Margin zwischen Ebene und Punkte in Kauf. Bei einem tiefen Wert für C sucht der Classifier nach einem möglichst hohen margin, nimmt aber falsche Klassifizierungen im trianing set in Kauf.\n",
    "\n",
    "### Bestimmung Parameter C\n",
    "Evaluation mit 2000 samples im test set:\n",
    "* C=0.1: Precision 0.777777777778, Recall 0.21875\n",
    "* C=0.2: Precision 0.785714285714, Recall 0.34375\n",
    "* C=0.3: Precision 0.769230769231, Recall 0.3125\n",
    "* C=0.4: Precision 0.714285714286, Recall 0.3125\n",
    "* C=0.5: Precision 0.666666666667, Recall 0.3125\n",
    "* C=0.6: Precision 0.555555555556, Recall 0.3125\n",
    "* C=0.7: Precision 0.555555555556, Recall 0.3125\n",
    "* C=0.8: Precision 0.578947368421, Recall 0.34375\n",
    "* C=0.9: Precision 0.578947368421, Recall 0.34375\n",
    "* C=1: Precision 0.578947368421, Recall 0.34375\n",
    "\n",
    "### Test mit Parameter C\n",
    "#### Test mit Matrix V1\n",
    "C=0.2\n",
    "* Samples 1000: Precision 1.0, Recall 0.240740740741\n",
    "* Samples 2000: Precision 0.833333333333, Recall 0.37037037037\n",
    "* Samples 5000: Precision 0.785714285714, Recall 0.407407407407\n",
    "* Samples 10000: Precision 0.821428571429, Recall 0.425925925926\n",
    "* Samples 15000: Precision 0.896551724138, Recall 0.481481481481\n",
    "\n",
    "#### Test mit Matrix V2 \n",
    "C=0.2\n",
    "* Samples 1000: Precision 1.0, Recall 0.327272727273\n",
    "* Samples 2000: Precision 0.9375, Recall 0.545454545455\n",
    "* Samples 5000: Precision 0.885714285714, Recall 0.563636363636\n",
    "* Samples 10000: Precision 0.861111111111, Recall 0.563636363636\n",
    "* Samples 15000: Precision 0.916666666667, Recall 0.6\n",
    "\n",
    "Samples=15000\n",
    "* C=0.2: Precision 0.916666666667, Recall 0.6\n",
    "* C=0.6: Precision 0.888888888889, Recall 0.727272727273\n",
    "* C=1.0: Precision 0.816326530612, Recall 0.727272727273\n",
    "\n",
    "#### Test mit 1/3 und 2/3 bei 15000 Samples\n",
    "* #1: Precision 0.955922865014, Recall 0.805104408353\n",
    "* #2: Precision 0.937555753791, Recall 0.812838360402\n",
    "* #3: Precision 0.956014362657, Recall 0.823665893271\n",
    "* #4: Precision 0.950628366248, Recall 0.819025522042\n",
    "* #5: Precision 0.946654611212, Recall 0.809744779582\n",
    "\n",
    "#### Test mit Matrix V3\n",
    "* #1: Precision 0.89219858156, Recall 0.643807574207\n",
    "\n",
    "#### Test mit Klassifikationsmatrix\n",
    "* #1: Precision 0.89248434238, Recall 0.707196029777\n",
    "* #2: Precision 0.887804878049, Recall 0.752688172043\n",
    "* #3 Precision 0.897177419355, Recall 0.736145574855\n",
    "* #4 Precision 0.881592039801, Recall 0.732837055418\n",
    "* #5 Precision 0.893551688843, Recall 0.722084367246\n",
    "* #6 Precision 0.892966360856, Recall 0.724565756824\n",
    "* #7 Precision 0.89296333003, Recall 0.745244003309\n",
    "* #8 Precision 0.895895895896, Recall 0.740281224152\n",
    "* #9 Precision 0.886294416244, Recall 0.722084367246\n",
    "* #10 Precision 0.896586345382, Recall 0.738626964433\n",
    "* #11 Precision 0.8921859545, Recall 0.746071133168\n",
    "* #12 Precision 0.908163265306, Recall 0.736145574855\n",
    "* #13 Precision 0.880445795339, Recall 0.718775847808\n",
    "* #14 Precision 0.890816326531, Recall 0.722084367246\n",
    "* #15 Precision 0.907237512742, Recall 0.736145574855\n",
    "------ sum ----\n",
    "Precision: sum: 13.3961655713 durchschnitt: 0.89307770475\n",
    "Recall: sum: 10.9809760132 durchschnitt: 0.732065067549"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Anzahl Samples: 19550\n",
      "Anzahl verwundbare Samples: 3628\n",
      "Anzahl nicht verwundbare Samples: 15922\n",
      "* #3 Precision 0.897177419355, Recall 0.736145574855\n",
      "* #4 Precision 0.881592039801, Recall 0.732837055418\n",
      "* #5 Precision 0.893551688843, Recall 0.722084367246\n",
      "* #6 Precision 0.892966360856, Recall 0.724565756824\n",
      "* #7 Precision 0.89296333003, Recall 0.745244003309\n",
      "* #8 Precision 0.895895895896, Recall 0.740281224152\n",
      "* #9 Precision 0.886294416244, Recall 0.722084367246\n",
      "* #10 Precision 0.896586345382, Recall 0.738626964433\n",
      "* #11 Precision 0.8921859545, Recall 0.746071133168\n",
      "* #12 Precision 0.908163265306, Recall 0.736145574855\n",
      "* #13 Precision 0.880445795339, Recall 0.718775847808\n",
      "* #14 Precision 0.890816326531, Recall 0.722084367246\n",
      "* #15 Precision 0.907237512742, Recall 0.736145574855\n",
      "------ sum ----\n",
      "Precision: sum: 13.3961655713 durchschnitt: 0.89307770475\n",
      "Recall: sum: 10.9809760132 durchschnitt: 0.732065067549\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "\n",
    "from imports.sparse_reader import get_matrices\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Read pickle\n",
    "feature_matrix = get_matrices('data/feature_matrix_sparse_classification.pickle')[0]\n",
    "\n",
    "#feature_matrix = feature_matrix[:1000, :]\n",
    "\n",
    "samples_count = feature_matrix.shape[0]\n",
    "features_count = feature_matrix.shape[1] - 1\n",
    "\n",
    "# Create own matrices for vulenrable and not vulnerable components\n",
    "vulnerable = (feature_matrix[np.where(feature_matrix[:,-1] != 0),:])[0]\n",
    "not_vulnerable = (feature_matrix[np.where(feature_matrix[:,-1] == 0),:])[0]\n",
    "\n",
    "precision_sum = 0.89248434238 + 0.887804878049\n",
    "recall_sum = 0.707196029777 + 0.752688172043\n",
    "print(\"Total Anzahl Samples: {}\".format(samples_count))\n",
    "print(\"Anzahl verwundbare Samples: {}\".format(vulnerable.shape[0]))\n",
    "print(\"Anzahl nicht verwundbare Samples: {}\".format(not_vulnerable.shape[0]))\n",
    "\n",
    "vulnerable_count = vulnerable.shape[0]\n",
    "not_vulnerable_count = not_vulnerable.shape[0]\n",
    "\n",
    "\n",
    "for i in [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]:\n",
    "    # Split into training sets (2/3) and test sets (1/3)\n",
    "    not_vulnerable_training = not_vulnerable[np.random.choice(not_vulnerable.shape[0], (not_vulnerable_count * 2 / 3), replace=False), :]\n",
    "    not_vulnerable_test = not_vulnerable[np.random.choice(not_vulnerable.shape[0], (not_vulnerable_count / 3), replace=False), :]\n",
    "\n",
    "    vulnerable_training = vulnerable[np.random.choice(vulnerable.shape[0], (vulnerable_count * 2 / 3), replace=False), :]\n",
    "    vulnerable_test = vulnerable[np.random.choice(vulnerable.shape[0], (vulnerable_count / 3), replace=False), :]\n",
    "\n",
    "    # Concatenate vulnerable and not vulnerable matrices\n",
    "    training_set = np.concatenate((not_vulnerable_training, vulnerable_training), axis=0)\n",
    "    test_set = np.concatenate((not_vulnerable_test, vulnerable_test), axis=0)\n",
    "\n",
    "    # Split sets into data and target\n",
    "    training_data = training_set[:, range(features_count)]\n",
    "    training_target = training_set[:, features_count]\n",
    "    training_target[training_target > 1] = 1\n",
    "\n",
    "    test_data = test_set[:, range(features_count)]\n",
    "    test_target = test_set[:, features_count]\n",
    "    test_target[test_target > 1] = 1\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Create classifier\n",
    "    clf = svm.SVC(kernel='linear', C=0.2)\n",
    "\n",
    "    # Fit model\n",
    "    clf.fit(training_data, training_target)\n",
    "\n",
    "    # Predict remaining data\n",
    "    target_prediction = clf.predict(test_data)\n",
    "\n",
    "    # Compute Precision-Recall\n",
    "    precision, recall, thresholds = precision_recall_curve(test_target, target_prediction)\n",
    "\n",
    "    precision_sum += precision[1]\n",
    "    recall_sum += recall[1]\n",
    "\n",
    "    # Print Precision and Recall\n",
    "    print('* #{} Precision {}, Recall {}'.format(i, precision[1], recall[1]))\n",
    "\n",
    "precision_schnitt = precision_sum / 15\n",
    "recall_schnitt = recall_sum / 15\n",
    "print('------ sum ----')\n",
    "print('Precision: sum: {} durchschnitt: {}'.format(precision_sum, precision_schnitt))\n",
    "print('Recall: sum: {} durchschnitt: {}'.format(recall_sum, recall_schnitt))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Anzahl Samples: 19550\n",
      "Anzahl verwundbare Samples: 3628\n",
      "Anzahl nicht verwundbare Samples: 15922\n",
      "------------------------------\n",
      "Prozentsatz verwundbare Komponenten im Testset:       18.6%\n",
      "Prozentsatz verwundbare Komponenten im Prediction Vektor:       15.7%\n",
      "------------------------------\n",
      "Precision 0.887804878049, Recall 0.752688172043\n",
      "time:       39.5min\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12468f890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "\n",
    "from imports.sparse_reader import get_matrices\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Read pickle\n",
    "feature_matrix = get_matrices('data/feature_matrix_sparse_classification.pickle')[0]\n",
    "\n",
    "#feature_matrix = feature_matrix[:1000, :]\n",
    "\n",
    "samples_count = feature_matrix.shape[0]\n",
    "features_count = feature_matrix.shape[1] - 1\n",
    "\n",
    "# Create own matrices for vulenrable and not vulnerable components\n",
    "vulnerable = (feature_matrix[np.where(feature_matrix[:,-1] != 0),:])[0]\n",
    "not_vulnerable = (feature_matrix[np.where(feature_matrix[:,-1] == 0),:])[0]\n",
    "\n",
    "print(\"Total Anzahl Samples: {}\".format(samples_count))\n",
    "print(\"Anzahl verwundbare Samples: {}\".format(vulnerable.shape[0]))\n",
    "print(\"Anzahl nicht verwundbare Samples: {}\".format(not_vulnerable.shape[0]))\n",
    "\n",
    "vulnerable_count = vulnerable.shape[0]\n",
    "not_vulnerable_count = not_vulnerable.shape[0]\n",
    "\n",
    "# Split into training sets (2/3) and test sets (1/3)\n",
    "not_vulnerable_training = not_vulnerable[np.random.choice(not_vulnerable.shape[0], (not_vulnerable_count * 2 / 3), replace=False), :]\n",
    "not_vulnerable_test = not_vulnerable[np.random.choice(not_vulnerable.shape[0], (not_vulnerable_count / 3), replace=False), :]\n",
    "\n",
    "vulnerable_training = vulnerable[np.random.choice(vulnerable.shape[0], (vulnerable_count * 2 / 3), replace=False), :]\n",
    "vulnerable_test = vulnerable[np.random.choice(vulnerable.shape[0], (vulnerable_count / 3), replace=False), :]\n",
    "\n",
    "# Concatenate vulnerable and not vulnerable matrices\n",
    "training_set = np.concatenate((not_vulnerable_training, vulnerable_training), axis=0)\n",
    "test_set = np.concatenate((not_vulnerable_test, vulnerable_test), axis=0)\n",
    "\n",
    "# Split sets into data and target\n",
    "training_data = training_set[:, range(features_count)]\n",
    "training_target = training_set[:, features_count]\n",
    "training_target[training_target > 1] = 1\n",
    "\n",
    "test_data = test_set[:, range(features_count)]\n",
    "test_target = test_set[:, features_count]\n",
    "test_target[test_target > 1] = 1\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Create classifier\n",
    "clf = svm.SVC(kernel='linear', C=0.2)\n",
    "\n",
    "# Fit model\n",
    "clf.fit(training_data, training_target)\n",
    "\n",
    "# Predict remaining data\n",
    "target_prediction = clf.predict(test_data)\n",
    "\n",
    "vulnerable_target_test = (test_target[test_target > 0]).size * 100.0 / test_target.size\n",
    "vulnerable_target_prediction = (target_prediction[target_prediction > 0]).size * 100.0 / target_prediction.size\n",
    "print(\"------------------------------\")\n",
    "print(\"Prozentsatz verwundbare Komponenten im Testset: {0:10.1f}%\".format(vulnerable_target_test))\n",
    "print(\"Prozentsatz verwundbare Komponenten im Prediction Vektor: {0:10.1f}%\".format(vulnerable_target_prediction))\n",
    "print(\"------------------------------\")\n",
    "\n",
    "# Compute Precision-Recall\n",
    "precision, recall, thresholds = precision_recall_curve(test_target, target_prediction)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = (end - start) / 60\n",
    "\n",
    "# Print Precision and Recall\n",
    "print('Precision {}, Recall {}'.format(precision[1], recall[1]))\n",
    "print('time: {0:10.1f}min'.format(elapsed))\n",
    "# Plot precision-recall-curve\n",
    "plt.clf()\n",
    "plt.plot(recall, precision, lw=2, color='navy',\n",
    "         label='Precision-Recall curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-Recall')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
